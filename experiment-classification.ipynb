{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Classification #\n",
    "##################\n",
    "composers = (\"Bach\", \"Mozart\", \"Beethoven\", \"Debussy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn import svm, linear_model, naive_bayes, neural_network, neighbors, ensemble\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "import random, math\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from tqdm.notebook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mxl-list.txt', 'r') as f:\n",
    "    dataset = [piece.strip() for piece in f.readlines()]\n",
    "    \n",
    "    composer_datas = []\n",
    "    for composer in composers:\n",
    "        composer_data = [f for f in dataset if f.replace('-', '_').split('_')[0] == composer]\n",
    "        composer_datas.append(composer_data)\n",
    "        \n",
    "    '''bach_data = [f for f in dataset if f.replace('-', '_').split('_')[0] == 'bach']\n",
    "    beethoven_data = [f for f in dataset if f.replace('-', '_').split('_')[0] == 'beethoven']\n",
    "    debussy_data = [f for f in dataset if f.replace('-', '_').split('_')[0] == 'debussy']\n",
    "    scarlatti_data = [f for f in dataset if f.replace('-', '_').split('_')[0] == 'scarlatti']\n",
    "    victoria_data = [f for f in dataset if f.replace('-', '_').split('_')[0] == 'victoria']'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"with open('bach-chordsequence.txt', 'r') as f:\\n    BACH = [' '.join(piece.strip('[]\\n').split(', ')) for piece in f.readlines()]\\n    BACH = [(BACH[i], bach_data[i]) for i in range(len(BACH))]\\nwith open('beethoven-chordsequence.txt', 'r') as f:\\n    BEETHOVEN = [' '.join(piece.strip('[]\\n').split(', ')) for piece in f.readlines()]\\n    BEETHOVEN = [(BEETHOVEN[i], beethoven_data[i]) for i in range(len(BEETHOVEN))]\\nwith open('debussy-chordsequence.txt', 'r') as f:\\n    DEBUSSY = [' '.join(piece.strip('[]\\n').split(', ')) for piece in f.readlines()]\\n    DEBUSSY = [(DEBUSSY[i], debussy_data[i]) for i in range(len(DEBUSSY))]\\nwith open('scarlatti-chordsequence.txt', 'r') as f:\\n    SCARLATTI = [' '.join(piece.strip('[]\\n').split(', ')) for piece in f.readlines()]\\n    SCARLATTI = [(SCARLATTI[i], scarlatti_data[i]) for i in range(len(SCARLATTI))]\\nwith open('victoria-chordsequence.txt', 'r') as f:\\n    VICTORIA = [' '.join(piece.strip('[]\\n').split(', ')) for piece in f.readlines()]\\n    VICTORIA = [(VICTORIA[i], victoria_data[i]) for i in range(len(VICTORIA))]\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COMPOSERs = []\n",
    "for composer, composer_data in zip(composers, composer_datas):\n",
    "    with open(f'{composer}-chordsequence.txt', 'r') as f:\n",
    "        COMPOSER = [' '.join(piece.strip('[]\\n').split(', ')) for piece in f.readlines()]\n",
    "        COMPOSER = [(COMPOSER[i], composer_data[i]) for i in range(len(COMPOSER))]\n",
    "        COMPOSERs.append(COMPOSER)\n",
    "    \n",
    "'''with open('bach-chordsequence.txt', 'r') as f:\n",
    "    BACH = [' '.join(piece.strip('[]\\n').split(', ')) for piece in f.readlines()]\n",
    "    BACH = [(BACH[i], bach_data[i]) for i in range(len(BACH))]\n",
    "with open('beethoven-chordsequence.txt', 'r') as f:\n",
    "    BEETHOVEN = [' '.join(piece.strip('[]\\n').split(', ')) for piece in f.readlines()]\n",
    "    BEETHOVEN = [(BEETHOVEN[i], beethoven_data[i]) for i in range(len(BEETHOVEN))]\n",
    "with open('debussy-chordsequence.txt', 'r') as f:\n",
    "    DEBUSSY = [' '.join(piece.strip('[]\\n').split(', ')) for piece in f.readlines()]\n",
    "    DEBUSSY = [(DEBUSSY[i], debussy_data[i]) for i in range(len(DEBUSSY))]\n",
    "with open('scarlatti-chordsequence.txt', 'r') as f:\n",
    "    SCARLATTI = [' '.join(piece.strip('[]\\n').split(', ')) for piece in f.readlines()]\n",
    "    SCARLATTI = [(SCARLATTI[i], scarlatti_data[i]) for i in range(len(SCARLATTI))]\n",
    "with open('victoria-chordsequence.txt', 'r') as f:\n",
    "    VICTORIA = [' '.join(piece.strip('[]\\n').split(', ')) for piece in f.readlines()]\n",
    "    VICTORIA = [(VICTORIA[i], victoria_data[i]) for i in range(len(VICTORIA))]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ngrams(input_list, N=4):\n",
    "    return [' '.join(input_list[i:i+N]) for i in range(len(input_list)-N+1)]\n",
    "\n",
    "def ngrams_by_composer(composer): \n",
    "    for i in range(1,5):\n",
    "        ngrams = []\n",
    "        for piece in composer:\n",
    "            ngrams += find_ngrams(piece[0].split(' '), i)\n",
    "        print(len(ngrams), '{}-grams total;'.format(str(i)), len(set(ngrams)), 'unique')\n",
    "    print('-')\n",
    "\n",
    "def show_ngrams(composer_data, composer_name):\n",
    "    print(composer_name, ':', len(composer_data), 'pieces')\n",
    "    ngrams_by_composer(composer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bach : 50 pieces\n",
      "23473 1-grams total; 175 unique\n",
      "23423 2-grams total; 5064 unique\n",
      "23373 3-grams total; 12651 unique\n",
      "23323 4-grams total; 16163 unique\n",
      "-\n",
      "Mozart : 50 pieces\n",
      "37893 1-grams total; 177 unique\n",
      "37843 2-grams total; 5668 unique\n",
      "37793 3-grams total; 15811 unique\n",
      "37743 4-grams total; 21892 unique\n",
      "-\n",
      "Beethoven : 50 pieces\n",
      "39567 1-grams total; 179 unique\n",
      "39517 2-grams total; 7082 unique\n",
      "39467 3-grams total; 19873 unique\n",
      "39417 4-grams total; 26545 unique\n",
      "-\n",
      "Debussy : 50 pieces\n",
      "22281 1-grams total; 175 unique\n",
      "22231 2-grams total; 5794 unique\n",
      "22181 3-grams total; 13530 unique\n",
      "22131 4-grams total; 17142 unique\n",
      "-\n",
      "all composers : 200 pieces\n",
      "123214 1-grams total; 179 unique\n",
      "123014 2-grams total; 12371 unique\n",
      "122814 3-grams total; 51552 unique\n",
      "122614 4-grams total; 76962 unique\n",
      "-\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"show_ngrams(BACH,'bach')\\nshow_ngrams(BEETHOVEN,'beethoven')\\nshow_ngrams(DEBUSSY,'debussy')\\nshow_ngrams(SCARLATTI,'scarlatti')\\nshow_ngrams(VICTORIA, 'victoria')\\nshow_ngrams(BACH+BEETHOVEN+DEBUSSY+SCARLATTI+VICTORIA, 'all composers')\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for COMPOSER, composer in zip(COMPOSERs, composers):\n",
    "    show_ngrams(COMPOSER, composer)\n",
    "show_ngrams(sum(COMPOSERs, []), 'all composers')\n",
    "\n",
    "'''show_ngrams(BACH,'bach')\n",
    "show_ngrams(BEETHOVEN,'beethoven')\n",
    "show_ngrams(DEBUSSY,'debussy')\n",
    "show_ngrams(SCARLATTI,'scarlatti')\n",
    "show_ngrams(VICTORIA, 'victoria')\n",
    "show_ngrams(BACH+BEETHOVEN+DEBUSSY+SCARLATTI+VICTORIA, 'all composers')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Xy(composers, size=1):\n",
    "    if size >= 1: # use every rows\n",
    "        indices = [range(len(composer)) for composer in composers]\n",
    "    else:\n",
    "        indices = [random.sample(range(len(composer)), math.ceil(size*len(composer))) for composer in composers]\n",
    "\n",
    "    y = []\n",
    "    for i in range(len(composers)):\n",
    "        y += [i for n in range(len(indices[i]))]\n",
    "    \n",
    "    X = []\n",
    "    for i in range(len(composers)):\n",
    "        X += [composers[i][j] for j in indices[i]]\n",
    "    \n",
    "    return X, np.array(y, dtype='int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidate(X_tuple, y, classifiers, vectorizer, NGRAMRANGE, K=10, set_=False):    \n",
    "    for clf in classifiers:\n",
    "        clf.cm_sum = np.zeros([len(set(y)),len(set(y))], dtype='int16') \\\n",
    "                     if set_ else np.zeros([len(composers), len(composers)], dtype='int16')\n",
    "        clf.accuracies, clf.fones, clf.misclassified, clf.runningtime = [], [], [], []\n",
    "        clf.fones_micro, clf.fones_macro = [], []\n",
    "        clf.name = str(clf).split('(')[0]\n",
    "\n",
    "    X = np.array([piece[0] for piece in X_tuple])\n",
    "    filenames = np.array([piece[1] for piece in X_tuple])\n",
    "    #print(f'y={y}')\n",
    "    kf = KFold(n_splits=min(K,len(y)), shuffle=True)\n",
    "    for train_index, test_index in kf.split(y):\n",
    "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "        \n",
    "        vct = vectorizer.set_params(lowercase=False, token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\", ngram_range=NGRAMRANGE)\n",
    "        X_train_tfidf = vct.fit_transform(X_train)\n",
    "#         X_test_tfidf = tfidf.transform(X_test)  # i think this computes tf-idf values using the whole test set, but i want each piece to be treated separately\n",
    "        X_test_tfidf = sp.vstack([vct.transform(np.array([piece])) for piece in X_test])\n",
    "        \n",
    "        for clf in classifiers:\n",
    "            t = datetime.now()\n",
    "            clf.fit(X_train_tfidf, y_train)\n",
    "            y_pred = clf.predict(X_test_tfidf)\n",
    "            clf.runningtime.append((datetime.now()-t).total_seconds())\n",
    "            #print(clf, clf.cm_sum, y_test, y_pred, confusion_matrix(y_test, y_pred, range(1, len(composers)+1)), sep='\\n') ###################################################\n",
    "            print(set(y_test))\n",
    "            clf.cm_sum += confusion_matrix(y_test, y_pred, labels=set(y_test) if set_ else range(1, len(composers)+1))\n",
    "            clf.misclassified.append(test_index[np.where(y_test != y_pred)]) # http://stackoverflow.com/a/25570632\n",
    "            clf.accuracies.append(accuracy_score(y_test, y_pred))\n",
    "            clf.fones.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "            clf.fones_micro.append(f1_score(y_test, y_pred, average='micro'))\n",
    "            clf.fones_macro.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "    result = dict()\n",
    "    for clf in classifiers:\n",
    "        clf.misclassified = np.sort(np.hstack(clf.misclassified))\n",
    "        result[clf.name] = [clf.cm_sum, clf.accuracies, clf.fones, clf.misclassified, filenames[clf.misclassified], clf.runningtime, clf.fones_micro, clf.fones_macro]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_classifiers(composers, NGRAMRANGES, classifiers, vectorizer, n=1, retrieve_title=True, set_=False):\n",
    "    misclassified_list = []\n",
    "    for NGRAMRANGE in tqdm(NGRAMRANGES, leave=False):\n",
    "        print('n-gram range', NGRAMRANGE)\n",
    "        X, y = build_Xy(composers, size=n)\n",
    "        cv_result = crossvalidate(X, y, classifiers, vectorizer, NGRAMRANGE, set_=set_)\n",
    "        for clf, results in cv_result.items():\n",
    "            print(clf)\n",
    "            cm = results[0]\n",
    "            print(cm)\n",
    "            acc = results[1] # using two different f-measures, don't need this\n",
    "#             print('accuracy', round(np.mean(acc)*100,2), '({})'.format(round(np.std(acc, ddof=1)*100,2)))\n",
    "            fones = results[2] # weighted average, don't need this\n",
    "#             print('f1', round(np.mean(fones)*100,2), '({})'.format(round(np.std(fones, ddof=1)*100,2)), fones)\n",
    "            misclassified = results[3]\n",
    "            misclassified_filenames = results[4]\n",
    "            misclassified_list += list(misclassified_filenames)\n",
    "#             print('misclassified',[(misclassified[i], misclassified_filenames[i]) for i in range(len(misclassified))])\n",
    "            runningtime = results[5]\n",
    "#             print('running time', np.sum(runningtime))\n",
    "            fones_micro = results[6]\n",
    "            fones_macro = results[7]\n",
    "            print('micro-averaged f-score (std) & macro-averaged f-score (std)')\n",
    "            print(round(np.mean(fones_micro),4), '({})'.format(round(np.std(fones_micro, ddof=1),4)), '&', round(np.mean(fones_macro),4), '({})'.format(round(np.std(fones_macro, ddof=1),4)))\n",
    "    print('-----')\n",
    "    return misclassified_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''COMPOSERS = [BACH, BEETHOVEN, DEBUSSY, SCARLATTI, VICTORIA]'''\n",
    "NGRAMRANGES = [(1,1),(2,2),(3,3),(4,4),(1,2),(3,4),(1,4)] #?\n",
    "CLASSIFIERS = [svm.LinearSVC(penalty='l2', C=5, loss='hinge'),\n",
    "               linear_model.LogisticRegression(penalty='l2', C=100, tol=1, multi_class='multinomial', solver='sag'),\n",
    "               neighbors.KNeighborsClassifier(weights='distance'),\n",
    "               naive_bayes.MultinomialNB(alpha=0.00001, fit_prior=False),\n",
    "               neural_network.MLPClassifier(solver='lbfgs',hidden_layer_sizes=(10,))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different methods of vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2491d5106da45c9b096a0b7714dfcdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-gram range (1, 1)\n",
      "y=[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "LinearSVC\n",
      "[[26  9  2  0]\n",
      " [13 21  9  0]\n",
      " [ 2  4 41  0]\n",
      " [ 0  0  0  0]]\n",
      "micro-averaged f-score (std) & macro-averaged f-score (std)\n",
      "0.625 (0.092) & 0.5853 (0.0922)\n",
      "LogisticRegression\n",
      "[[16 13  4  0]\n",
      " [12 17 14  0]\n",
      " [ 2  5 36  0]\n",
      " [ 0  0  0  0]]\n",
      "micro-averaged f-score (std) & macro-averaged f-score (std)\n",
      "0.475 (0.1061) & 0.4435 (0.0923)\n",
      "KNeighborsClassifier\n",
      "[[26  9  1  0]\n",
      " [20 20  2  0]\n",
      " [ 1 12 33  0]\n",
      " [ 0  0  0  0]]\n",
      "micro-averaged f-score (std) & macro-averaged f-score (std)\n",
      "0.575 (0.089) & 0.5508 (0.1225)\n",
      "MultinomialNB\n",
      "[[22 13  0  0]\n",
      " [12 20  5  0]\n",
      " [ 3  6 37  0]\n",
      " [ 0  0  0  0]]\n",
      "micro-averaged f-score (std) & macro-averaged f-score (std)\n",
      "0.525 (0.1087) & 0.4893 (0.1222)\n",
      "MLPClassifier\n",
      "[[23 17  1  0]\n",
      " [11 29  3  0]\n",
      " [ 2  8 39  0]\n",
      " [ 0  0  0  0]]\n",
      "micro-averaged f-score (std) & macro-averaged f-score (std)\n",
      "0.62 (0.1111) & 0.5959 (0.1176)\n",
      "n-gram range (2, 2)\n",
      "y=[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "LinearSVC\n",
      "[[31  6  1  0]\n",
      " [15 26  5  0]\n",
      " [ 0  1 48  0]\n",
      " [ 0  0  0  0]]\n",
      "micro-averaged f-score (std) & macro-averaged f-score (std)\n",
      "0.715 (0.0914) & 0.6733 (0.0836)\n",
      "LogisticRegression\n",
      "[[20 19  1  0]\n",
      " [12 25  5  0]\n",
      " [ 2  6 39  0]\n",
      " [ 0  0  0  0]]\n",
      "micro-averaged f-score (std) & macro-averaged f-score (std)\n",
      "0.56 (0.0459) & 0.5341 (0.0637)\n",
      "KNeighborsClassifier\n",
      "[[27  5  0  0]\n",
      " [19 19  2  0]\n",
      " [ 2  2 37  0]\n",
      " [ 0  0  0  0]]\n",
      "micro-averaged f-score (std) & macro-averaged f-score (std)\n",
      "0.61 (0.1542) & 0.5823 (0.1533)\n",
      "MultinomialNB\n",
      "[[18 27  0  0]\n",
      " [ 7 34  4  0]\n",
      " [ 0 16 33  0]\n",
      " [ 0  0  0  0]]\n",
      "micro-averaged f-score (std) & macro-averaged f-score (std)\n",
      "0.525 (0.0979) & 0.4905 (0.0976)\n",
      "MLPClassifier\n",
      "[[25 15  0  0]\n",
      " [ 9 30  4  0]\n",
      " [ 2  7 39  0]\n",
      " [ 0  0  0  0]]\n",
      "micro-averaged f-score (std) & macro-averaged f-score (std)\n",
      "0.655 (0.1165) & 0.615 (0.1263)\n",
      "n-gram range (3, 3)\n",
      "y=[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "LinearSVC\n",
      "[[31  8  1  0]\n",
      " [13 23  8  0]\n",
      " [ 0  2 47  0]\n",
      " [ 0  0  0  0]]\n",
      "micro-averaged f-score (std) & macro-averaged f-score (std)\n",
      "0.66 (0.0775) & 0.6286 (0.0624)\n",
      "LogisticRegression\n",
      "[[29  8  0  0]\n",
      " [16 21  7  0]\n",
      " [ 3  3 43  0]\n",
      " [ 0  0  0  0]]\n",
      "micro-averaged f-score (std) & macro-averaged f-score (std)\n",
      "0.585 (0.0883) & 0.5701 (0.099)\n",
      "KNeighborsClassifier\n",
      "[[24  5  0  0]\n",
      " [12 22  0  0]\n",
      " [ 0  2 38  0]\n",
      " [ 0  0  0  0]]\n",
      "micro-averaged f-score (std) & macro-averaged f-score (std)\n",
      "0.63 (0.1006) & 0.6167 (0.0934)\n",
      "MultinomialNB\n",
      "[[33 11  0  0]\n",
      " [13 28  1  0]\n",
      " [ 2  9 37  0]\n",
      " [ 0  0  0  0]]\n",
      "micro-averaged f-score (std) & macro-averaged f-score (std)\n",
      "0.6 (0.1027) & 0.5905 (0.0889)\n",
      "MLPClassifier\n",
      "[[25 14  1  0]\n",
      " [ 6 25  7  0]\n",
      " [ 0  4 44  0]\n",
      " [ 0  0  0  0]]\n",
      "micro-averaged f-score (std) & macro-averaged f-score (std)\n",
      "0.595 (0.1212) & 0.5682 (0.1142)\n",
      "n-gram range (4, 4)\n",
      "y=[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n"
     ]
    }
   ],
   "source": [
    "VECTORIZER = TfidfVectorizer(sublinear_tf=True)\n",
    "benchmark_classifiers(COMPOSERs,NGRAMRANGES,CLASSIFIERS,VECTORIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTORIZER = CountVectorizer(binary=True)\n",
    "benchmark_classifiers(COMPOSERs,NGRAMRANGES,CLASSIFIERS,VECTORIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTORIZER = CountVectorizer()\n",
    "benchmark_classifiers(COMPOSERs,NGRAMRANGES,CLASSIFIERS,VECTORIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test pairwise classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    NGRAMRANGES = [(1,2)]\n",
    "    VECTORIZER = TfidfVectorizer(sublinear_tf=True)\n",
    "    for indices in tqdm(combinations(range(len(composers)),2)):\n",
    "        print('composer indices',[i for i in indices]) # COMPOSERs = [BACH, BEETHOVEN, DEBUSSY, SCARLATTI, VICTORIA]\n",
    "        benchmark_classifiers([COMPOSERs[i] for i in indices],NGRAMRANGES,CLASSIFIERS,VECTORIZER, set_=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identify the often-misclassified files\n",
    "# do the experiment 100 times with the best classifier, SVM\n",
    "# then find pieces that are misclassified more than 50% of the time\n",
    "'''COMPOSERS = [BACH, BEETHOVEN, DEBUSSY, SCARLATTI, VICTORIA]'''\n",
    "CLASSIFIERS = [svm.LinearSVC(penalty='l2', C=5, loss='hinge')] # \n",
    "appendix = []\n",
    "for i in trange(100):\n",
    "    appendix += benchmark_classifiers(COMPOSERs,NGRAMRANGES,CLASSIFIERS,VECTORIZER)\n",
    "Counter(appendix).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use both chord sequences and duration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "with open('bach-durations.txt', 'r') as f:\n",
    "    BD2 = [line.strip() for line in f.readlines()]\n",
    "with open('beethoven-durations.txt', 'r') as f:\n",
    "    BD = [line.strip() for line in f.readlines()]\n",
    "with open('debussy-durations.txt', 'r') as f:\n",
    "    DD = [line.strip() for line in f.readlines()]\n",
    "with open('scarlatti-durations.txt', 'r') as f:\n",
    "    SD = [line.strip() for line in f.readlines()]\n",
    "with open('victoria-durations.txt', 'r') as f:\n",
    "    VD = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "BD2_TYPELENGTH = [piece.split(';') for piece in BD2]\n",
    "BD_TYPELENGTH = [piece.split(';') for piece in BD]\n",
    "DD_TYPELENGTH = [piece.split(';') for piece in DD]\n",
    "SD_TYPELENGTH = [piece.split(';') for piece in SD]\n",
    "VD_TYPELENGTH = [piece.split(';') for piece in VD]\n",
    "\n",
    "typelengths = list(set(flatten(BD2_TYPELENGTH+BD_TYPELENGTH+DD_TYPELENGTH+SD_TYPELENGTH+VD_TYPELENGTH)))\n",
    "typelength_dict = {typelengths[i]: str(i+300) for i in range(len(typelengths))}\n",
    "BD2_T = [(' '.join([typelength_dict[dur] for dur in piece]),'temp') for piece in BD2_TYPELENGTH]\n",
    "BD_T = [(' '.join([typelength_dict[dur] for dur in piece]),'temp') for piece in BD_TYPELENGTH]\n",
    "DD_T = [(' '.join([typelength_dict[dur] for dur in piece]),'temp') for piece in DD_TYPELENGTH]\n",
    "SD_T = [(' '.join([typelength_dict[dur] for dur in piece]),'temp') for piece in SD_TYPELENGTH]\n",
    "VD_T = [(' '.join([typelength_dict[dur] for dur in piece]),'temp') for piece in VD_TYPELENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print most common duration by composer, regardless of element type(chord/note/rest)\n",
    "\n",
    "# BD_LENGTHONLY = [[string.split('|')[1] for string in piece.split(';')] for piece in BD]\n",
    "# SD_LENGTHONLY = [[string.split('|')[1] for string in piece.split(';')] for piece in SD]\n",
    "# BD2_LENGTHONLY = [[string.split('|')[1] for string in piece.split(';')] for piece in BD2]\n",
    "# DD_LENGTHONLY = [[string.split('|')[1] for string in piece.split(';')] for piece in DD]\n",
    "# VD_LENGTHONLY = [[string.split('|')[1] for string in piece.split(';')] for piece in VD]\n",
    "# lengths = list(set(flatten(BD2_LENGTHONLY+BD_LENGTHONLY+DD_LENGTHONLY+SD_LENGTHONLY+VD_LENGTHONLY)))\n",
    "# length_dict = {lengths[i]: str(i+200) for i in range(len(lengths))}\n",
    "# BD_L = [(' '.join([length_dict[dur] for dur in piece]),'temp') for piece in BD_LENGTHONLY]\n",
    "# BD2_L = [(' '.join([length_dict[dur] for dur in piece]),'temp') for piece in BD2_LENGTHONLY]\n",
    "# SD_L = [(' '.join([length_dict[dur] for dur in piece]),'temp') for piece in SD_LENGTHONLY]\n",
    "# DD_L = [(' '.join([length_dict[dur] for dur in piece]),'temp') for piece in DD_LENGTHONLY]\n",
    "# VD_L = [(' '.join([length_dict[dur] for dur in piece]),'temp') for piece in VD_LENGTHONLY]\n",
    "# duration_all       = flatten(BD2_LENGTHONLY+BD_LENGTHONLY+DD_LENGTHONLY+SD_LENGTHONLY+VD_LENGTHONLY)\n",
    "# duration_bach      = flatten(BD2_LENGTHONLY)\n",
    "# duration_beethoven = flatten(BD_LENGTHONLY)\n",
    "# duration_debussy   = flatten(DD_LENGTHONLY)\n",
    "# duration_scarlatti = flatten(SD_LENGTHONLY)\n",
    "# duration_victoria  = flatten(VD_LENGTHONLY)\n",
    "\n",
    "# for l in [duration_all,duration_bach,duration_beethoven,duration_debussy,duration_scarlatti,duration_victoria]:\n",
    "#     for key, value in Counter(l).most_common(10):\n",
    "#         print(key, '&', round(100*value/len(l),2))\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidate_twofeaturesets(X_tuple1, X_tuple2, y, classifiers, vectorizer, range1, range2, K=10):    \n",
    "    for clf in classifiers:\n",
    "        clf.cm_sum = np.zeros([len(set(y)),len(set(y))], dtype='int16')\n",
    "        clf.accuracies, clf.fones, clf.misclassified, clf.runningtime = [], [], [], []\n",
    "        clf.fones_micro, clf.fones_macro = [], []\n",
    "        clf.name = str(clf).split('(')[0]\n",
    "\n",
    "    X1 = np.array([piece[0] for piece in X_tuple1])\n",
    "    X2 = np.array([piece[0] for piece in X_tuple2])\n",
    "    filenames = np.array([piece[1] for piece in X_tuple2])\n",
    "    kf = KFold(n_splits=K, shuffle=True)\n",
    "    for train_index, test_index in kf.split(y):\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        X_train_new, X_test_new = X1[train_index], X1[test_index]\n",
    "        vct1 = vectorizer.set_params(ngram_range=range1)\n",
    "        X_train, X_test = X2[train_index], X2[test_index] \n",
    "        vct2 = vectorizer.set_params(ngram_range=range2)\n",
    "   \n",
    "        X_train_new_tfidf = vct1.fit_transform(X_train_new) # use two separate vectorizers for each feature set\n",
    "        X_test_new_tfidf = sp.vstack([vct1.transform(np.array([piece])) for piece in X_test_new])\n",
    "        X_train_tfidf = vct2.fit_transform(X_train)\n",
    "        X_test_tfidf = sp.vstack([vct2.transform(np.array([piece])) for piece in X_test])\n",
    "        \n",
    "        X_train_tfidf = sp.hstack((X_train_tfidf, X_train_new_tfidf)) # Merge the two feature sets\n",
    "        X_test_tfidf = sp.hstack((X_test_tfidf, X_test_new_tfidf))\n",
    "        \n",
    "        for clf in classifiers:\n",
    "            t = datetime.now()\n",
    "            clf.fit(X_train_tfidf, y_train)\n",
    "            y_pred = clf.predict(X_test_tfidf)\n",
    "            clf.runningtime.append((datetime.now()-t).total_seconds())\n",
    "            clf.cm_sum += confusion_matrix(y_test, y_pred, labels=range(1, len(composers)+1))\n",
    "            clf.misclassified.append(test_index[np.where(y_test != y_pred)]) # http://stackoverflow.com/a/25570632\n",
    "            clf.accuracies.append(accuracy_score(y_test, y_pred))\n",
    "            clf.fones.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "            clf.fones_micro.append(f1_score(y_test, y_pred, average='micro'))\n",
    "            clf.fones_macro.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "    result = dict()\n",
    "    for clf in classifiers:\n",
    "        clf.misclassified = np.sort(np.hstack(clf.misclassified))\n",
    "        result[clf.name] = [clf.cm_sum, clf.accuracies, clf.fones, clf.misclassified, filenames[clf.misclassified], clf.runningtime, clf.fones_micro, clf.fones_macro]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_classifiers_twofeaturesets(composers1, composers2, range1, range2, classifiers, vectorizer, n=1, retrieve_title=True):\n",
    "    misclassified_list = []\n",
    "    print('duration n-gram range:', range1, 'chord n-gram range:', range2)\n",
    "    X1, y = build_Xy(composers1, size=n)\n",
    "    X2, y = build_Xy(composers2, size=n)\n",
    "    cv_result = crossvalidate_twofeaturesets(X1, X2, y, classifiers, vectorizer, range1, range2)\n",
    "    for clf, results in cv_result.items():\n",
    "        print(clf)\n",
    "        cm = results[0]\n",
    "        print(cm)\n",
    "        acc = results[1]\n",
    "        fones = results[2]\n",
    "        misclassified = results[3]\n",
    "        misclassified_filenames = results[4]\n",
    "        misclassified_list += list(misclassified_filenames)\n",
    "#             print('misclassified',[(misclassified[i], misclassified_filenames[i]) for i in range(len(misclassified))])\n",
    "        runningtime = results[5]\n",
    "#         print('running time', np.sum(runningtime))\n",
    "        fones_micro = results[6]\n",
    "        fones_macro = results[7]\n",
    "        print('F-measures')\n",
    "        print(round(np.mean(fones_micro),4), '({})'.format(round(np.std(fones_micro, ddof=1),4)), '&', round(np.mean(fones_macro),4), '({})'.format(round(np.std(fones_macro, ddof=1),4)))\n",
    "    print('-----')\n",
    "    return misclassified_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFIERS = [svm.LinearSVC(penalty='l2', C=5, loss='hinge'),\n",
    "               linear_model.LogisticRegression(penalty='l2', C=100, tol=1, multi_class='multinomial', solver='sag'),\n",
    "               neighbors.KNeighborsClassifier(weights='distance'),\n",
    "               naive_bayes.MultinomialNB(alpha=0.00001, fit_prior=False),\n",
    "               neural_network.MLPClassifier(solver='lbfgs',hidden_layer_sizes=(10,))]\n",
    "VECTORIZER = TfidfVectorizer(sublinear_tf=True, lowercase=False, token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "\n",
    "COMPOSERS1 = [BD2_T,BD_T,DD_T,SD_T,VD_T]\n",
    "COMPOSERS2 = [BACH, BEETHOVEN, DEBUSSY, SCARLATTI, VICTORIA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_classifiers_twofeaturesets(COMPOSERS1, COMPOSERS2, (1,1), (1,2), CLASSIFIERS, VECTORIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for indices in combinations(range(5),2):\n",
    "    print('composer indices',[i for i in indices]) \n",
    "    benchmark_classifiers_twofeaturesets([COMPOSERS1[i] for i in indices],[COMPOSERS2[i] for i in indices],(1,1),(1,2),CLASSIFIERS,VECTORIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identify the often-misclassified files, using both feature sets\n",
    "# do the experiment 100 times with the best classifier, SVM\n",
    "# then find pieces that are misclassified more than 50% of the time\n",
    "COMPOSERS1 = [BD2_T,BD_T,DD_T,SD_T,VD_T]\n",
    "COMPOSERS2 = [BACH, BEETHOVEN, DEBUSSY, SCARLATTI, VICTORIA]\n",
    "CLASSIFIERS = [svm.LinearSVC(penalty='l2', C=5, loss='hinge')] # \n",
    "appendix = []\n",
    "for i in range(100):\n",
    "    appendix += benchmark_classifiers_twofeaturesets(COMPOSERS1, COMPOSERS2, (1,1), (1,2), CLASSIFIERS, VECTORIZER)\n",
    "Counter(appendix).most_common()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
