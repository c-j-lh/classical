{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Classification #\n",
    "##################\n",
    "composers = (\"Bach\", \"Mozart\", \"Beethoven\", \"Debussy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn import svm, linear_model, naive_bayes, neural_network, neighbors, ensemble\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "import random, math\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from tqdm.notebook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mxl-list.txt', 'r') as f:\n",
    "    dataset = [piece.strip() for piece in f.readlines()]\n",
    "    \n",
    "    composer_datas = []\n",
    "    for composer in composers:\n",
    "        composer_data = [f for f in dataset if f.replace('-', '_').split('_')[0] == composer]\n",
    "        composer_datas.append(composer_data)\n",
    "        \n",
    "    '''bach_data = [f for f in dataset if f.replace('-', '_').split('_')[0] == 'bach']\n",
    "    beethoven_data = [f for f in dataset if f.replace('-', '_').split('_')[0] == 'beethoven']\n",
    "    debussy_data = [f for f in dataset if f.replace('-', '_').split('_')[0] == 'debussy']\n",
    "    scarlatti_data = [f for f in dataset if f.replace('-', '_').split('_')[0] == 'scarlatti']\n",
    "    victoria_data = [f for f in dataset if f.replace('-', '_').split('_')[0] == 'victoria']'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"with open('bach-chordsequence.txt', 'r') as f:\\n    BACH = [' '.join(piece.strip('[]\\n').split(', ')) for piece in f.readlines()]\\n    BACH = [(BACH[i], bach_data[i]) for i in range(len(BACH))]\\nwith open('beethoven-chordsequence.txt', 'r') as f:\\n    BEETHOVEN = [' '.join(piece.strip('[]\\n').split(', ')) for piece in f.readlines()]\\n    BEETHOVEN = [(BEETHOVEN[i], beethoven_data[i]) for i in range(len(BEETHOVEN))]\\nwith open('debussy-chordsequence.txt', 'r') as f:\\n    DEBUSSY = [' '.join(piece.strip('[]\\n').split(', ')) for piece in f.readlines()]\\n    DEBUSSY = [(DEBUSSY[i], debussy_data[i]) for i in range(len(DEBUSSY))]\\nwith open('scarlatti-chordsequence.txt', 'r') as f:\\n    SCARLATTI = [' '.join(piece.strip('[]\\n').split(', ')) for piece in f.readlines()]\\n    SCARLATTI = [(SCARLATTI[i], scarlatti_data[i]) for i in range(len(SCARLATTI))]\\nwith open('victoria-chordsequence.txt', 'r') as f:\\n    VICTORIA = [' '.join(piece.strip('[]\\n').split(', ')) for piece in f.readlines()]\\n    VICTORIA = [(VICTORIA[i], victoria_data[i]) for i in range(len(VICTORIA))]\""
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COMPOSERs = []\n",
    "for composer, composer_data in zip(composers, composer_datas):\n",
    "    with open(f'{composer}-chordsequence.txt', 'r') as f:\n",
    "        COMPOSER = [' '.join(piece.strip('[]\\n').split(', ')) for piece in f.readlines()]\n",
    "        COMPOSER = [(COMPOSER[i], composer_data[i]) for i in range(len(COMPOSER))]\n",
    "        COMPOSERs.append(COMPOSER)\n",
    "    \n",
    "'''with open('bach-chordsequence.txt', 'r') as f:\n",
    "    BACH = [' '.join(piece.strip('[]\\n').split(', ')) for piece in f.readlines()]\n",
    "    BACH = [(BACH[i], bach_data[i]) for i in range(len(BACH))]\n",
    "with open('beethoven-chordsequence.txt', 'r') as f:\n",
    "    BEETHOVEN = [' '.join(piece.strip('[]\\n').split(', ')) for piece in f.readlines()]\n",
    "    BEETHOVEN = [(BEETHOVEN[i], beethoven_data[i]) for i in range(len(BEETHOVEN))]\n",
    "with open('debussy-chordsequence.txt', 'r') as f:\n",
    "    DEBUSSY = [' '.join(piece.strip('[]\\n').split(', ')) for piece in f.readlines()]\n",
    "    DEBUSSY = [(DEBUSSY[i], debussy_data[i]) for i in range(len(DEBUSSY))]\n",
    "with open('scarlatti-chordsequence.txt', 'r') as f:\n",
    "    SCARLATTI = [' '.join(piece.strip('[]\\n').split(', ')) for piece in f.readlines()]\n",
    "    SCARLATTI = [(SCARLATTI[i], scarlatti_data[i]) for i in range(len(SCARLATTI))]\n",
    "with open('victoria-chordsequence.txt', 'r') as f:\n",
    "    VICTORIA = [' '.join(piece.strip('[]\\n').split(', ')) for piece in f.readlines()]\n",
    "    VICTORIA = [(VICTORIA[i], victoria_data[i]) for i in range(len(VICTORIA))]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ngrams(input_list, N=4):\n",
    "    return [' '.join(input_list[i:i+N]) for i in range(len(input_list)-N+1)]\n",
    "\n",
    "def ngrams_by_composer(composer): \n",
    "    for i in range(1,5):\n",
    "        ngrams = []\n",
    "        for piece in composer:\n",
    "            ngrams += find_ngrams(piece[0].split(' '), i)\n",
    "        print(len(ngrams), '{}-grams total;'.format(str(i)), len(set(ngrams)), 'unique')\n",
    "    print('-')\n",
    "\n",
    "def show_ngrams(composer_data, composer_name):\n",
    "    print(composer_name, ':', len(composer_data), 'pieces')\n",
    "    ngrams_by_composer(composer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bach : 50 pieces\n",
      "23473 1-grams total; 175 unique\n",
      "23423 2-grams total; 5064 unique\n",
      "23373 3-grams total; 12651 unique\n",
      "23323 4-grams total; 16163 unique\n",
      "-\n",
      "Mozart : 50 pieces\n",
      "37893 1-grams total; 177 unique\n",
      "37843 2-grams total; 5668 unique\n",
      "37793 3-grams total; 15811 unique\n",
      "37743 4-grams total; 21892 unique\n",
      "-\n",
      "Beethoven : 50 pieces\n",
      "39567 1-grams total; 179 unique\n",
      "39517 2-grams total; 7082 unique\n",
      "39467 3-grams total; 19873 unique\n",
      "39417 4-grams total; 26545 unique\n",
      "-\n",
      "Debussy : 50 pieces\n",
      "22281 1-grams total; 175 unique\n",
      "22231 2-grams total; 5794 unique\n",
      "22181 3-grams total; 13530 unique\n",
      "22131 4-grams total; 17142 unique\n",
      "-\n",
      "all composers : 200 pieces\n",
      "123214 1-grams total; 179 unique\n",
      "123014 2-grams total; 12371 unique\n",
      "122814 3-grams total; 51552 unique\n",
      "122614 4-grams total; 76962 unique\n",
      "-\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"show_ngrams(BACH,'bach')\\nshow_ngrams(BEETHOVEN,'beethoven')\\nshow_ngrams(DEBUSSY,'debussy')\\nshow_ngrams(SCARLATTI,'scarlatti')\\nshow_ngrams(VICTORIA, 'victoria')\\nshow_ngrams(BACH+BEETHOVEN+DEBUSSY+SCARLATTI+VICTORIA, 'all composers')\""
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for COMPOSER, composer in zip(COMPOSERs, composers):\n",
    "    show_ngrams(COMPOSER, composer)\n",
    "show_ngrams(sum(COMPOSERs, []), 'all composers')\n",
    "\n",
    "'''show_ngrams(BACH,'bach')\n",
    "show_ngrams(BEETHOVEN,'beethoven')\n",
    "show_ngrams(DEBUSSY,'debussy')\n",
    "show_ngrams(SCARLATTI,'scarlatti')\n",
    "show_ngrams(VICTORIA, 'victoria')\n",
    "show_ngrams(BACH+BEETHOVEN+DEBUSSY+SCARLATTI+VICTORIA, 'all composers')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Xy(composers, size=1):\n",
    "    if size >= 1: # use every rows\n",
    "        indices = [range(len(composer)) for composer in composers]\n",
    "    else:\n",
    "        indices = [random.sample(range(len(composer)), math.ceil(size*len(composer))) for composer in composers]\n",
    "\n",
    "    y = []\n",
    "    for i in range(len(composers)):\n",
    "        y += [i for n in range(len(indices[i]))]\n",
    "    \n",
    "    X = []\n",
    "    for i in range(len(composers)):\n",
    "        X += [composers[i][j] for j in indices[i]]\n",
    "    \n",
    "    return X, np.array(y, dtype='int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidate(X_tuple, y, classifiers, vectorizer, NGRAMRANGE, K=10, set_=False):    \n",
    "    for clf in classifiers:\n",
    "        clf.cm_sum = np.zeros([len(set(y)),len(set(y))], dtype='int16') \\\n",
    "                     if set_ else np.zeros([len(composers), len(composers)], dtype='int16')\n",
    "        clf.accuracies, clf.fones, clf.misclassified, clf.runningtime = [], [], [], []\n",
    "        clf.fones_micro, clf.fones_macro = [], []\n",
    "        clf.name = str(clf).split('(')[0]\n",
    "\n",
    "    X = np.array([piece[0] for piece in X_tuple])\n",
    "    \n",
    "    import pickle #\n",
    "    ids = [int(piece[1][:-4].split('_')[1]) for piece in X_tuple] #\n",
    "    with open('indices.pickle', 'rb') as file:\n",
    "        df = pickle.load(file)\n",
    "    indices = np.asarray([df.loc[id_] for id_ in ids])\n",
    "    #X = np.array([list(ai) + list(bi) for ai, bi in zip(X, indices)]) #\n",
    "    \n",
    "    filenames = np.array([piece[1] for piece in X_tuple])\n",
    "    #print(f'y={y}')\n",
    "    kf = KFold(n_splits=min(K,len(y)), shuffle=True)\n",
    "    for train_index, test_index in kf.split(y):\n",
    "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "        vct = vectorizer.set_params(lowercase=False, token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\", ngram_range=NGRAMRANGE)\n",
    "        X_train_tfidf = vct.fit_transform(X_train)\n",
    "#         X_test_tfidf = tfidf.transform(X_test)  # i think this computes tf-idf values using the whole test set, but i want each piece to be treated separately\n",
    "        X_test_tfidf = sp.vstack([vct.transform(np.array([piece])) for piece in X_test])\n",
    "        \n",
    "        for clf in classifiers:\n",
    "            t = datetime.now()\n",
    "            print(X_test_tfidf.toarray()[0])\n",
    "            #X_train_tfidf = np.array([list(ai) + list(bi) for ai, bi in zip(X_train_tfidf.toarray(), indices)])\n",
    "            #X_test_tfidf = np.array([list(ai) + list(bi) for ai, bi in zip(X_test_tfidf.toarray(), indices)])\n",
    "            clf.fit(X_train_tfidf, y_train)\n",
    "            y_pred = clf.predict(X_test_tfidf)\n",
    "            clf.runningtime.append((datetime.now()-t).total_seconds())\n",
    "            clf.cm_sum += confusion_matrix(y_test, y_pred, labels=set(y_test) if set_ else range(1, len(composers)+1))\n",
    "            clf.misclassified.append(test_index[np.where(y_test != y_pred)]) # http://stackoverflow.com/a/25570632\n",
    "            clf.accuracies.append(accuracy_score(y_test, y_pred))\n",
    "            clf.fones.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "            clf.fones_micro.append(f1_score(y_test, y_pred, average='micro'))\n",
    "            clf.fones_macro.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "    result = dict()\n",
    "    for clf in classifiers:\n",
    "        clf.misclassified = np.sort(np.hstack(clf.misclassified))\n",
    "        result[clf.name] = [clf.cm_sum, clf.accuracies, clf.fones, clf.misclassified, filenames[clf.misclassified], clf.runningtime, clf.fones_micro, clf.fones_macro]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803b438d7138486ba782f672e9360318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-gram range (1, 1)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "(20, 178)\n",
      "LinearSVC\n",
      "[[25 10  3  0]\n",
      " [12 17 10  0]\n",
      " [ 1  3 42  0]\n",
      " [ 0  0  0  0]]\n",
      "micro-averaged f-score (std) & macro-averaged f-score (std)\n",
      "0.6 (0.0882) & 0.5462 (0.1015)\n",
      "LogisticRegression\n",
      "[[22 18  2  0]\n",
      " [ 8 27 10  0]\n",
      " [ 1 14 33  0]\n",
      " [ 0  0  0  0]]\n",
      "micro-averaged f-score (std) & macro-averaged f-score (std)\n",
      "0.535 (0.1454) & 0.4815 (0.1743)\n",
      "KNeighborsClassifier\n",
      "[[24 12  1  0]\n",
      " [21 17  3  0]\n",
      " [ 0 15 31  0]\n",
      " [ 0  0  0  0]]\n",
      "micro-averaged f-score (std) & macro-averaged f-score (std)\n",
      "0.545 (0.0864) & 0.5198 (0.0746)\n",
      "MultinomialNB\n",
      "[[23 11  0  0]\n",
      " [10 21  5  0]\n",
      " [ 3  3 40  0]\n",
      " [ 0  0  0  0]]\n",
      "micro-averaged f-score (std) & macro-averaged f-score (std)\n",
      "0.555 (0.1117) & 0.5285 (0.1046)\n",
      "MLPClassifier\n",
      "[[25 18  1  0]\n",
      " [ 9 23  7  0]\n",
      " [ 2  8 40  0]\n",
      " [ 0  0  0  0]]\n",
      "micro-averaged f-score (std) & macro-averaged f-score (std)\n",
      "0.595 (0.0832) & 0.5466 (0.0905)\n",
      "n-gram range (2, 2)\n",
      "(20, 11909)\n",
      "(20, 11909)\n",
      "(20, 11909)\n",
      "(20, 11909)\n",
      "(20, 11909)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-169-79929180cc10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mVECTORIZER\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msublinear_tf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbenchmark_classifiers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCOMPOSERs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNGRAMRANGES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mCLASSIFIERS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mVECTORIZER\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-101-dd73c7b5cbf9>\u001b[0m in \u001b[0;36mbenchmark_classifiers\u001b[1;34m(composers, NGRAMRANGES, classifiers, vectorizer, n, retrieve_title, set_)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'n-gram range'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNGRAMRANGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_Xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomposers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mcv_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrossvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNGRAMRANGE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcv_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-168-810683e1b39e>\u001b[0m in \u001b[0;36mcrossvalidate\u001b[1;34m(X_tuple, y, classifiers, vectorizer, NGRAMRANGE, K, set_)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m#print(f'x\\'s shape is {X_test.shape}')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mvct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_pattern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu\"(?u)\\\\b\\\\w+\\\\b\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNGRAMRANGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mX_train_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;31m#         X_test_tfidf = tfidf.transform(X_test)  # i think this computes tf-idf values using the whole test set, but i want each piece to be treated separately\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mX_test_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpiece\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpiece\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1857\u001b[0m         \"\"\"\n\u001b[0;32m   1858\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1859\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1860\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1861\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1220\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1129\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m                 \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_word_ngrams\u001b[1;34m(self, tokens, stop_words)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m_word_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m         \u001b[1;34m\"\"\"Turn tokens into a sequence of n-grams after stop words filtering\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[1;31m# handle stop words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "VECTORIZER = TfidfVectorizer(sublinear_tf=True)\n",
    "benchmark_classifiers(COMPOSERs,NGRAMRANGES,CLASSIFIERS,VECTORIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_classifiers(composers, NGRAMRANGES, classifiers, vectorizer, n=1, retrieve_title=True, set_=False):\n",
    "    misclassified_list = []\n",
    "    for NGRAMRANGE in tqdm(NGRAMRANGES, leave=False):\n",
    "        print('n-gram range', NGRAMRANGE)\n",
    "        X, y = build_Xy(composers, size=n)\n",
    "        cv_result = crossvalidate(X, y, classifiers, vectorizer, NGRAMRANGE, set_=set_)\n",
    "        for clf, results in cv_result.items():\n",
    "            print(clf)\n",
    "            cm = results[0]\n",
    "            print(cm)\n",
    "            acc = results[1] # using two different f-measures, don't need this\n",
    "#             print('accuracy', round(np.mean(acc)*100,2), '({})'.format(round(np.std(acc, ddof=1)*100,2)))\n",
    "            fones = results[2] # weighted average, don't need this\n",
    "#             print('f1', round(np.mean(fones)*100,2), '({})'.format(round(np.std(fones, ddof=1)*100,2)), fones)\n",
    "            misclassified = results[3]\n",
    "            misclassified_filenames = results[4]\n",
    "            misclassified_list += list(misclassified_filenames)\n",
    "#             print('misclassified',[(misclassified[i], misclassified_filenames[i]) for i in range(len(misclassified))])\n",
    "            runningtime = results[5]\n",
    "#             print('running time', np.sum(runningtime))\n",
    "            fones_micro = results[6]\n",
    "            fones_macro = results[7]\n",
    "            print('micro-averaged f-score (std) & macro-averaged f-score (std)')\n",
    "            print(round(np.mean(fones_micro),4), '({})'.format(round(np.std(fones_micro, ddof=1),4)), '&', round(np.mean(fones_macro),4), '({})'.format(round(np.std(fones_macro, ddof=1),4)))\n",
    "    print('-----')\n",
    "    return misclassified_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''COMPOSERS = [BACH, BEETHOVEN, DEBUSSY, SCARLATTI, VICTORIA]'''\n",
    "NGRAMRANGES = [(1,1),(2,2),(3,3),(4,4),(1,2),(3,4),(1,4)] #?\n",
    "CLASSIFIERS = [svm.LinearSVC(penalty='l2', C=5, loss='hinge'),\n",
    "               linear_model.LogisticRegression(penalty='l2', C=100, tol=1, multi_class='multinomial', solver='sag'),\n",
    "               neighbors.KNeighborsClassifier(weights='distance'),\n",
    "               naive_bayes.MultinomialNB(alpha=0.00001, fit_prior=False),\n",
    "               neural_network.MLPClassifier(solver='lbfgs',hidden_layer_sizes=(10,))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different methods of vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTORIZER = CountVectorizer(binary=True)\n",
    "benchmark_classifiers(COMPOSERs,NGRAMRANGES,CLASSIFIERS,VECTORIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTORIZER = CountVectorizer()\n",
    "benchmark_classifiers(COMPOSERs,NGRAMRANGES,CLASSIFIERS,VECTORIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test pairwise classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    NGRAMRANGES = [(1,2)]\n",
    "    VECTORIZER = TfidfVectorizer(sublinear_tf=True)\n",
    "    for indices in tqdm(list(combinations(range(len(composers)),2))):\n",
    "        print('composer indices',[i for i in indices]) # COMPOSERs = [BACH, BEETHOVEN, DEBUSSY, SCARLATTI, VICTORIA]\n",
    "        benchmark_classifiers([COMPOSERs[i] for i in indices],NGRAMRANGES,CLASSIFIERS,VECTORIZER, set_=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identify the often-misclassified files\n",
    "# do the experiment 100 times with the best classifier, SVM\n",
    "# then find pieces that are misclassified more than 50% of the time\n",
    "'''COMPOSERS = [BACH, BEETHOVEN, DEBUSSY, SCARLATTI, VICTORIA]'''\n",
    "CLASSIFIERS = [svm.LinearSVC(penalty='l2', C=5, loss='hinge')] # \n",
    "appendix = []\n",
    "for i in trange(100):\n",
    "    appendix += benchmark_classifiers(COMPOSERs,NGRAMRANGES,CLASSIFIERS,VECTORIZER)\n",
    "Counter(appendix).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use both chord sequences and duration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "with open('Bach-durations.txt', 'r') as f:\n",
    "    BA = [line.strip() for line in f.readlines()]\n",
    "with open('Mozart-durations.txt', 'r') as f:\n",
    "    MO = [line.strip() for line in f.readlines()]\n",
    "with open('Beethoven-durations.txt', 'r') as f:\n",
    "    BE = [line.strip() for line in f.readlines()]\n",
    "with open('Debussy-durations.txt', 'r') as f:\n",
    "    DE = [line.strip() for line in f.readlines()]\n",
    "'''with open('victoria-durations.txt', 'r') as f:\n",
    "    VD = [line.strip() for line in f.readlines()]'''\n",
    "    \n",
    "#BD2_TYPELENGTH = [piece.split(';') for piece in BD2]\n",
    "BA_TYPELENGTH = [piece.split(';') for piece in BA]\n",
    "MO_TYPELENGTH = [piece.split(';') for piece in MO]\n",
    "BE_TYPELENGTH = [piece.split(';') for piece in BE]\n",
    "DE_TYPELENGTH = [piece.split(';') for piece in DE]\n",
    "\n",
    "typelengths = list(set(flatten(BA_TYPELENGTH + MO_TYPELENGTH + BE_TYPELENGTH + DE_TYPELENGTH)))\n",
    "typelength_dict = {typelengths[i]: str(i+300) for i in range(len(typelengths))}\n",
    "BA_T = [(' '.join([typelength_dict[dur] for dur in piece]),'temp') for piece in BA_TYPELENGTH]\n",
    "MO_T = [(' '.join([typelength_dict[dur] for dur in piece]),'temp') for piece in MO_TYPELENGTH]\n",
    "BE_T = [(' '.join([typelength_dict[dur] for dur in piece]),'temp') for piece in BE_TYPELENGTH]\n",
    "DE_T = [(' '.join([typelength_dict[dur] for dur in piece]),'temp') for piece in DE_TYPELENGTH]\n",
    "#VD_T = [(' '.join([typelength_dict[dur] for dur in piece]),'temp') for piece in VD_TYPELENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print most common duration by composer, regardless of element type(chord/note/rest)\n",
    "\n",
    "# BD_LENGTHONLY = [[string.split('|')[1] for string in piece.split(';')] for piece in BD]\n",
    "# SD_LENGTHONLY = [[string.split('|')[1] for string in piece.split(';')] for piece in SD]\n",
    "# BD2_LENGTHONLY = [[string.split('|')[1] for string in piece.split(';')] for piece in BD2]\n",
    "# DD_LENGTHONLY = [[string.split('|')[1] for string in piece.split(';')] for piece in DD]\n",
    "# VD_LENGTHONLY = [[string.split('|')[1] for string in piece.split(';')] for piece in VD]\n",
    "# lengths = list(set(flatten(BD2_LENGTHONLY+BD_LENGTHONLY+DD_LENGTHONLY+SD_LENGTHONLY+VD_LENGTHONLY)))\n",
    "# length_dict = {lengths[i]: str(i+200) for i in range(len(lengths))}\n",
    "# BD_L = [(' '.join([length_dict[dur] for dur in piece]),'temp') for piece in BD_LENGTHONLY]\n",
    "# BD2_L = [(' '.join([length_dict[dur] for dur in piece]),'temp') for piece in BD2_LENGTHONLY]\n",
    "# SD_L = [(' '.join([length_dict[dur] for dur in piece]),'temp') for piece in SD_LENGTHONLY]\n",
    "# DD_L = [(' '.join([length_dict[dur] for dur in piece]),'temp') for piece in DD_LENGTHONLY]\n",
    "# VD_L = [(' '.join([length_dict[dur] for dur in piece]),'temp') for piece in VD_LENGTHONLY]\n",
    "# duration_all       = flatten(BD2_LENGTHONLY+BD_LENGTHONLY+DD_LENGTHONLY+SD_LENGTHONLY+VD_LENGTHONLY)\n",
    "# duration_bach      = flatten(BD2_LENGTHONLY)\n",
    "# duration_beethoven = flatten(BD_LENGTHONLY)\n",
    "# duration_debussy   = flatten(DD_LENGTHONLY)\n",
    "# duration_scarlatti = flatten(SD_LENGTHONLY)\n",
    "# duration_victoria  = flatten(VD_LENGTHONLY)\n",
    "\n",
    "# for l in [duration_all,duration_bach,duration_beethoven,duration_debussy,duration_scarlatti,duration_victoria]:\n",
    "#     for key, value in Counter(l).most_common(10):\n",
    "#         print(key, '&', round(100*value/len(l),2))\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidate_twofeaturesets(X_tuple1, X_tuple2, y, classifiers, vectorizer, range1, range2, K=10):    \n",
    "    for clf in classifiers:\n",
    "        clf.cm_sum = np.zeros([len(set(y)),len(set(y))], dtype='int16')\n",
    "        clf.accuracies, clf.fones, clf.misclassified, clf.runningtime = [], [], [], []\n",
    "        clf.fones_micro, clf.fones_macro = [], []\n",
    "        clf.name = str(clf).split('(')[0]\n",
    "\n",
    "    X1 = np.array([piece[0] for piece in X_tuple1])\n",
    "    X2 = np.array([piece[0] for piece in X_tuple2])\n",
    "    filenames = np.array([piece[1] for piece in X_tuple2])\n",
    "    kf = KFold(n_splits=K, shuffle=True)\n",
    "    for train_index, test_index in kf.split(y):\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        X_train_new, X_test_new = X1[train_index], X1[test_index]\n",
    "        vct1 = vectorizer.set_params(ngram_range=range1)\n",
    "        X_train, X_test = X2[train_index], X2[test_index] \n",
    "        vct2 = vectorizer.set_params(ngram_range=range2)\n",
    "   \n",
    "        X_train_new_tfidf = vct1.fit_transform(X_train_new) # use two separate vectorizers for each feature set\n",
    "        X_test_new_tfidf = sp.vstack([vct1.transform(np.array([piece])) for piece in X_test_new])\n",
    "        X_train_tfidf = vct2.fit_transform(X_train)\n",
    "        X_test_tfidf = sp.vstack([vct2.transform(np.array([piece])) for piece in X_test])\n",
    "        \n",
    "        X_train_tfidf = sp.hstack((X_train_tfidf, X_train_new_tfidf)) # Merge the two feature sets\n",
    "        X_test_tfidf = sp.hstack((X_test_tfidf, X_test_new_tfidf))\n",
    "        \n",
    "        for clf in classifiers:\n",
    "            t = datetime.now()\n",
    "            clf.fit(X_train_tfidf, y_train)\n",
    "            y_pred = clf.predict(X_test_tfidf)\n",
    "            clf.runningtime.append((datetime.now()-t).total_seconds())\n",
    "            clf.cm_sum += confusion_matrix(y_test, y_pred)#, labels=range(1, len(composers)+1))\n",
    "            clf.misclassified.append(test_index[np.where(y_test != y_pred)]) # http://stackoverflow.com/a/25570632\n",
    "            clf.accuracies.append(accuracy_score(y_test, y_pred))\n",
    "            clf.fones.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "            clf.fones_micro.append(f1_score(y_test, y_pred, average='micro'))\n",
    "            clf.fones_macro.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "    result = dict()\n",
    "    for clf in classifiers:\n",
    "        clf.misclassified = np.sort(np.hstack(clf.misclassified))\n",
    "        result[clf.name] = [clf.cm_sum, clf.accuracies, clf.fones, clf.misclassified, filenames[clf.misclassified], clf.runningtime, clf.fones_micro, clf.fones_macro]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_classifiers_twofeaturesets(composers1, composers2, range1, range2, classifiers, vectorizer, n=1, retrieve_title=True):\n",
    "    misclassified_list = []\n",
    "    print('duration n-gram range:', range1, 'chord n-gram range:', range2)\n",
    "    X1, y = build_Xy(composers1, size=n)\n",
    "    X2, y = build_Xy(composers2, size=n)\n",
    "    cv_result = crossvalidate_twofeaturesets(X1, X2, y, classifiers, vectorizer, range1, range2)\n",
    "    for clf, results in cv_result.items():\n",
    "        print(clf)\n",
    "        cm = results[0]\n",
    "        print(cm)\n",
    "        acc = results[1]\n",
    "        fones = results[2]\n",
    "        misclassified = results[3]\n",
    "        misclassified_filenames = results[4]\n",
    "        misclassified_list += list(misclassified_filenames)\n",
    "#             print('misclassified',[(misclassified[i], misclassified_filenames[i]) for i in range(len(misclassified))])\n",
    "        runningtime = results[5]\n",
    "#         print('running time', np.sum(runningtime))\n",
    "        fones_micro = results[6]\n",
    "        fones_macro = results[7]\n",
    "        print('F-measures')\n",
    "        print(round(np.mean(fones_micro),4), '({})'.format(round(np.std(fones_micro, ddof=1),4)), '&', round(np.mean(fones_macro),4), '({})'.format(round(np.std(fones_macro, ddof=1),4)))\n",
    "    print('-----')\n",
    "    return misclassified_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFIERS = [svm.LinearSVC(penalty='l2', C=5, loss='hinge'),\n",
    "               linear_model.LogisticRegression(penalty='l2', C=100, tol=1, multi_class='multinomial', solver='sag'),\n",
    "               neighbors.KNeighborsClassifier(weights='distance'),\n",
    "               naive_bayes.MultinomialNB(alpha=0.00001, fit_prior=False),\n",
    "               neural_network.MLPClassifier(solver='lbfgs',hidden_layer_sizes=(10,))]\n",
    "VECTORIZER = TfidfVectorizer(sublinear_tf=True, lowercase=False, token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "\n",
    "COMPOSERS1 = [BA_T, MO_T, BE_T, DE_T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_classifiers_twofeaturesets(COMPOSERS1, COMPOSERs, (1,1), (1,2), CLASSIFIERS, VECTORIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for indices in combinations(range(len(composers)),2):\n",
    "    print('composer indices', [i for i in indices]) \n",
    "    benchmark_classifiers_twofeaturesets([COMPOSERS1[i] for i in indices],[COMPOSERs[i] for i in indices],(1,1),(1,2),CLASSIFIERS,VECTORIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identify the often-misclassified files, using both feature sets\n",
    "# do the experiment 100 times with the best classifier, SVM\n",
    "# then find pieces that are misclassified more than 50% of the time\n",
    "COMPOSERS1 = [BA_T, MO_T, BE_T, DE_T]\n",
    "#COMPOSERS2 = [BACH, BEETHOVEN, DEBUSSY, SCARLATTI, VICTORIA]\n",
    "CLASSIFIERS = [svm.LinearSVC(penalty='l2', C=5, loss='hinge')] # \n",
    "appendix = []\n",
    "for i in trange(100):\n",
    "    appendix += benchmark_classifiers_twofeaturesets(COMPOSERS1, COMPOSERs, (1,1), (1,2), CLASSIFIERS, VECTORIZER)\n",
    "Counter(appendix).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = build_Xy(COMPOSERs, size=1)\n",
    "print(X)\n",
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
